{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDAlsAj4XV7A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from csv_data import HousePricesDatasetWrapper\n",
        "\n",
        "wrapper = HousePricesDatasetWrapper()\n",
        "train_data, valid_data, test_data = wrapper.get_flat_datasets()"
      ],
      "metadata": {
        "id": "6dFrEksSXXZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the train predictors\n",
        "train_data[0][:2]"
      ],
      "metadata": {
        "id": "L_IJAgT8X2t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the train target\n",
        "train_data[1][:2]"
      ],
      "metadata": {
        "id": "Ov7BWEVwYNOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a class for creating Dense layers\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class Dense():\n",
        "  def __init__(self,input_size,output_size,activation=True,seed=0): # Activation always set as true, set seed as zero for initializing random weight matrices\n",
        "    self.add_activation=activation\n",
        "    self.hidden=None\n",
        "    self.prev_hidden=None # Hidden and previous hidden will be eventually used for creating a \"Fully Connected Dense Neural Network\"\n",
        "    # Will be using ReLu activation function hence using LeCun Normal intializing strategy for weight matrices\n",
        "    np.random.seed(seed)\n",
        "    k=math.sqrt(1/input_size)\n",
        "    self.weights = (k**2) * np.random.randn(input_size,output_size) + 0\n",
        "\n",
        "    # Intializing bias to 1\n",
        "    self.bias=np.ones((1,output_size))\n",
        "\n",
        "  # Defining forward propagation function\n",
        "  def forward(self,x):\n",
        "    self.prev_hidden=x.copy()\n",
        "    x=np.matmul(x,self.weights) + self.bias\n",
        "\n",
        "    if self.add_activation:\n",
        "      x=np.maximum(x,0)\n",
        "\n",
        "    self.hidden=x.copy()\n",
        "    return x\n",
        "\n",
        "  # Defining the backward propagation function\n",
        "  def backward(self,grad): # Grad is the gradient for the current layer\n",
        "    # Undo ing the activation function (ReLu)\n",
        "    if self.add_activation:\n",
        "      grad=np.matmul(grad,np.heaviside(self.hidden,0))\n",
        "\n",
        "    # Calculating the weight and bias gradient\n",
        "    w_grad=self.prev_hidden.T @ grad\n",
        "    b_grad = np.mean(grad, axis=0)\n",
        "    param_grads = [w_grad, b_grad]\n",
        "\n",
        "    grad= grad @ self.weights.T\n",
        "    return param_grads, grad\n",
        "\n",
        "  # defining function to update the weights based on the gradient\n",
        "\n",
        "  def update(self,w_grad,b_grad):\n",
        "    self.weights += w_grad\n",
        "    self.bias += b_grad\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GltEhNiHHO8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 layer neural network - 7 inputs (dimension) making 25 hidden features, second layer makes 10 hidden features and then the last layer gives one output\n",
        "layers = [\n",
        "    Dense(7, 25),\n",
        "    Dense(25, 10),\n",
        "    Dense(10, 1, activation=False)\n",
        "]"
      ],
      "metadata": {
        "id": "cqgZlMGjVnPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x, layers):\n",
        "    # Loop through each layer\n",
        "    for layer in layers:\n",
        "        # Run the forward pass\n",
        "        x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "def backward(grad, layers):\n",
        "    # Save the gradients for each layer\n",
        "    layer_grads = []\n",
        "    # Loop through each layer in reverse order (starting from the output layer)\n",
        "    for layer in reversed(layers):\n",
        "        # Get the parameter gradients and the next layer gradient\n",
        "        param_grads, grad = layer.backward(grad)\n",
        "        layer_grads.append(param_grads)\n",
        "    return layer_grads\n"
      ],
      "metadata": {
        "id": "HRECP0z8VojB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}